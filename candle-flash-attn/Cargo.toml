[package]
name = "candle-flash-attn"
version.workspace = true
edition.workspace = true

description = "Flash attention layer for the candle ML framework."
repository.workspace = true
keywords.workspace = true
categories.workspace = true
license.workspace = true
readme = "README.md"

[lib]
name = "candle_flash_attn"
path = "src/lib.rs"
crate-type = ["rlib"]

[dependencies]
candle = { path = "../candle-core", features = [
    "cuda",
], package = "candle-core" }
half = { workspace = true }

[build-dependencies]
bindgen_cuda = { workspace = true }
anyhow = { workspace = true }

[dev-dependencies]
anyhow = { workspace = true }
candle-nn = { path = "../candle-nn", features = ["cuda"] }

[features]
default = []
cudnn = ["candle/cudnn"] # Enable cuDNN with CUDA
