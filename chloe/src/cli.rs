use clap::Parser;
use tokenizers::Tokenizer;
use anyhow::Result;
use crate::config::default::Config;

#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
pub struct Args {
    /// GGUF file to load, typically a .gguf file generated by the quantize command from llama.cpp
    #[arg(long)]
    pub model: Option<String>,

    /// The initial prompt file path, relative to config location if config is used
    #[arg(long)]
    pub prompt: Option<String>,

    /// The length of the sample to generate (in tokens).
    #[arg(short = 'n', long)]
    pub sample_len: Option<usize>,

    /// The tokenizer config in json format.
    #[arg(long)]
    pub tokenizer: Option<String>,

    /// The temperature used to generate samples, use 0 for greedy sampling.
    #[arg(long)]
    pub temperature: Option<f64>,

    /// Nucleus sampling probability cutoff.
    #[arg(long)]
    pub top_p: Option<f64>,

    /// Only sample among the top K samples.
    #[arg(long)]
    pub top_k: Option<usize>,

    /// The seed to use when generating random samples.
    #[arg(long)]
    pub seed: Option<u64>,

    /// Penalty to be applied for repeating tokens, 1. means no penalty.
    #[arg(long)]
    pub repeat_penalty: Option<f32>,

    /// The context size to consider for the repeat penalty.
    #[arg(long)]
    pub repeat_last_n: Option<usize>,

    /// Generate a default config.toml file (destructive: overwrites existing)
    #[arg(long)]
    pub generate_config: bool,

    /// Generate a default prompt.md file (destructive: overwrites existing)
    #[arg(long)]
    pub generate_prompt: bool,

    /// Specify a specific config.toml file to use
    #[arg(long)]
    pub use_config: Option<String>,

    /// Specify a specific prompt file to use
    #[arg(long)]
    pub use_prompt: Option<String>,

    /// Specify a specific model file to use
    #[arg(long)]
    pub use_model: Option<String>,

    /// Specify a specific tokenizer file to use
    #[arg(long)]
    pub use_tokenizer: Option<String>,

    /// Enable tracing (generates a trace-timestamp.json file).
    #[arg(long)]
    pub tracing: bool,

    /// Process prompt elements separately.
    #[arg(long)]
    pub split_prompt: bool,

    /// Run on CPU rather than GPU even if a GPU is available.
    #[arg(long)]
    pub cpu: bool,

    /// Run in chat mode.
    #[arg(long)]
    pub chat: bool,

    /// Run in code mode.
    #[arg(long)]
    pub code: bool,

    /// Run training conversion.
    #[arg(long)]
    pub training: bool,

    /// Run in translate mode.
    #[arg(long)]
    pub translate: bool,
}

impl Args {
    pub fn tokenizer(&self, config: &Config, config_dir: Option<&std::path::Path>) -> Result<Tokenizer> {
        let tokenizer_path = self.effective_tokenizer_path(config, config_dir);
        Tokenizer::from_file(&tokenizer_path).map_err(anyhow::Error::msg)
    }

    pub async fn prompt_content(&self, config: &Config, config_dir: Option<&std::path::Path>) -> Result<String> {
        let prompt_path = self.effective_prompt_path(config, config_dir);
        crate::markdown::read_markdown(&prompt_path).await
    }

    pub fn tokenizer_path(&self, config: &Config, config_dir: Option<&std::path::Path>) -> String {
        self.effective_tokenizer_path(config, config_dir)
    }

    pub fn model_path(&self, config: &Config, config_dir: Option<&std::path::Path>) -> String {
        self.effective_model_path(config, config_dir)
    }

    fn effective_tokenizer_path(&self, config: &Config, config_dir: Option<&std::path::Path>) -> String {
        let path = if let Some(path) = &self.use_tokenizer {
            path.clone()
        } else if let Some(path) = &self.tokenizer {
            path.clone()
        } else {
            self.resolve_path(&config.tokenizer, config_dir)
        };
        if !std::path::Path::new(&path).exists() {
            panic!("Tokenizer file does not exist: {}", path);
        }
        path
    }

    fn effective_model_path(&self, config: &Config, config_dir: Option<&std::path::Path>) -> String {
        let path = if let Some(path) = &self.use_model {
            path.clone()
        } else if let Some(path) = &self.model {
            path.clone()
        } else {
            self.resolve_path(&config.model, config_dir)
        };
        if !std::path::Path::new(&path).exists() {
            panic!("Model file does not exist: {}", path);
        }
        path
    }

    fn effective_prompt_path(&self, config: &Config, config_dir: Option<&std::path::Path>) -> String {
        let path = if let Some(path) = &self.use_prompt {
            path.clone()
        } else if let Some(path) = &self.prompt {
            path.clone()
        } else {
            self.resolve_path(&config.prompt, config_dir)
        };
        if !std::path::Path::new(&path).exists() {
            panic!("Prompt file does not exist: {}", path);
        }
        path
    }

    fn resolve_path(&self, relative_path: &str, config_dir: Option<&std::path::Path>) -> String {
        if let Some(dir) = config_dir {
            dir.join(relative_path).to_string_lossy().to_string()
        } else {
            relative_path.to_string()
        }
    }

    pub fn effective_sample_len(&self, config: &Config) -> usize {
        self.sample_len.unwrap_or(config.sample_len)
    }

    pub fn effective_temperature(&self, config: &Config) -> f64 {
        self.temperature.unwrap_or(config.temperature)
    }

    pub fn effective_top_p(&self, config: &Config) -> Option<f64> {
        self.top_p.or(config.top_p)
    }

    pub fn effective_top_k(&self, config: &Config) -> Option<usize> {
        self.top_k.or(config.top_k)
    }

    pub fn effective_seed(&self, config: &Config) -> u64 {
        self.seed.unwrap_or(config.seed)
    }

    pub fn effective_repeat_penalty(&self, config: &Config) -> f32 {
        self.repeat_penalty.unwrap_or(config.repeat_penalty)
    }

    pub fn effective_repeat_last_n(&self, config: &Config) -> usize {
        self.repeat_last_n.unwrap_or(config.repeat_last_n)
    }
}